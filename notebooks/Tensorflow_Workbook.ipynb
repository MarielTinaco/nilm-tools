{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np      \n",
    "\n",
    "#from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adinilm.utils import paths_manager as pathsman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_profile = pathsman.PROFILES_DIR / \"unetnilm_ukdale_20240321_155419\"\n",
    "\n",
    "print(f\"Selected profile {selected_profile.resolve()} {'exists' if selected_profile.exists() else 'does not exist'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoise_inputs = np.load(selected_profile / \"training\" / \"denoise_inputs.npy\")\n",
    "noise_inputs = np.load(selected_profile / \"training\" / \"noise_inputs.npy\")\n",
    "targets = np.load(selected_profile / \"training\" / \"targets.npy\")\n",
    "states = np.load(selected_profile / \"training\" / \"states.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "\tsplit_1 = int(0.60 * len(data))\n",
    "\tsplit_2 = int(0.85 * len(data))\n",
    "\ttrain = data[:split_1]\n",
    "\tvalidation = data[split_1:split_2]\n",
    "\ttest = data[split_2:]\n",
    "\treturn train, validation, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataLoader(tf.keras.utils.Sequence):\n",
    "\n",
    "        class_dict = {\"fridge\" : 0, \"washer dryer\" : 1, \"kettle\" : 2, \"dish washer\" : 3, \"microwave\" : 4}\n",
    "\n",
    "        def __init__(self, profile_path, d_type, batch_size=256, seq_len=100, denoise=True, *args, **kwargs):\n",
    "                super(CustomDataLoader, self).__init__(*args, **kwargs)\n",
    "\n",
    "                self.profile_path = Path(profile_path)\n",
    "                self.batch_size = batch_size\n",
    "                self.seq_len = seq_len\n",
    "                self.d_type = d_type \n",
    "                self.denoise = denoise\n",
    "                self.data, self.labels = self.__load()\n",
    "                self.indices = np.arange(self.data.shape[0])\n",
    "\n",
    "        def __load(self):\n",
    "\n",
    "                if self.denoise:\n",
    "                        x = np.load(self.profile_path / \"training\" / \"denoise_inputs.npy\")\n",
    "                else:\n",
    "                        x = np.load(self.profile_path / \"training\" / \"noise_inputs.npy\")\n",
    "                y = np.load(self.profile_path / \"training\" / \"targets.npy\")\n",
    "                z = np.load(self.profile_path / \"training\" / \"states.npy\")\n",
    "\n",
    "                train_x, val_x, test_x = split_data(x)\n",
    "                train_y, val_y, test_y = split_data(y)\n",
    "                train_z, val_z, test_z = split_data(z)          \n",
    "\n",
    "                if self.d_type == \"train\":\n",
    "                        x = train_x\n",
    "                        y = train_y\n",
    "                        z = train_z\n",
    "                elif self.d_type == \"test\":\n",
    "                        x = test_x\n",
    "                        y = test_y\n",
    "                        z = test_z\n",
    "                else:\n",
    "                        x = val_x\n",
    "                        y = val_y\n",
    "                        z = val_z\n",
    "\n",
    "                return x, (z, y)\n",
    "\n",
    "        def __len__(self):\n",
    "                return (self.data.shape[0] - self.seq_len) // self.batch_size\n",
    "\n",
    "        def get_sample(self, index):\n",
    "                indices = self.indices[index : index + self.seq_len]\n",
    "                inds_inputs = sorted(indices[:self.seq_len])\n",
    "                inds_labels = sorted(indices[self.seq_len-1:self.seq_len])\n",
    "\t\t\n",
    "                states = self.labels[0]\n",
    "                power = self.labels[1]\n",
    "        \n",
    "                return self.data[inds_inputs], (states[inds_labels], power[inds_labels])\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "                batch = [self.get_sample(idx) for idx in range(index * self.batch_size,(index + 1) * self.batch_size)]\n",
    "                batch_data, batch_label = zip(*batch)\n",
    "                \n",
    "                batch_data = np.array(batch_data)\n",
    "                batch_label = np.array(batch_label)\n",
    "\n",
    "                batch_label = tf.squeeze(batch_label, axis=2)\n",
    "\n",
    "                return batch_data, tuple([batch_label[:,0,:], batch_label[:,1,:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 100\n",
    "denoise = True\n",
    "dloader = CustomDataLoader(selected_profile, \"train\", 256, seq_len=seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_window_length):\n",
    "\n",
    "    \"\"\"Specifies the structure of a seq2point model using Keras' functional API.\n",
    "\n",
    "    Returns:\n",
    "    model (tensorflow.keras.Model): The uncompiled seq2point model.\n",
    "\n",
    "    \"\"\"\n",
    "    input_layer = tf.keras.layers.Input(shape=(input_window_length,))\n",
    "    reshape_layer = tf.keras.layers.Reshape((1, input_window_length, 1))(input_layer)\n",
    "    conv_layer_1 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(10, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(reshape_layer)\n",
    "    conv_layer_2 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(8, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_1)\n",
    "    conv_layer_3 = tf.keras.layers.Convolution2D(filters=40, kernel_size=(6, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_2)\n",
    "    conv_layer_4 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_3)\n",
    "    conv_layer_5 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_4)\n",
    "    flatten_layer = tf.keras.layers.Flatten()(conv_layer_5)\n",
    "    label_layer = tf.keras.layers.Dense(1024, activation=\"relu\")(flatten_layer)\n",
    "    output_layer_1 = tf.keras.layers.Dense(2*5, activation=\"linear\", name=\"y1_output\")(label_layer)\n",
    "    output_layer_2 = tf.keras.layers.Dense(5*5, activation=\"linear\", name=\"y2_output\")(label_layer)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=[output_layer_1, output_layer_2])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resnet_model(input_window_length):\n",
    "    \n",
    "        input_layer = tf.keras.layers.Input(shape=(input_window_length,))\n",
    "        reshape_layer = tf.keras.layers.Reshape((1, input_window_length, 1))(input_layer)\n",
    "        \n",
    "        conv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 1), strides=(1, 1), padding=\"same\", activation=\"relu\", name='conv1')(reshape_layer)\n",
    "        bn1 = tf.keras.layers.BatchNormalization(name='bn1')(conv1)\n",
    "        conv2 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 1), strides=(1, 1), padding=\"same\", activation=\"relu\", name='conv2')(bn1)\n",
    "        bn2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "        conv3 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 1), strides=(1, 1), padding=\"same\", activation=\"relu\", name='bn2')(bn2)\n",
    "        resid_1 = tf.keras.layers.Add()([bn2, conv3])\n",
    "        relu1 = tf.keras.layers.ReLU(name='relu1')(resid_1)\n",
    "        bn_res1 = tf.keras.layers.BatchNormalization(name='bn_res1')(relu1)\n",
    "\n",
    "        conv4 = tf.keras.layers.Conv2D(filters=48, kernel_size=(3, 1), strides=(1, 1), padding=\"same\", activation=\"relu\", name='conv4')(bn_res1)\n",
    "        bn4 = tf.keras.layers.BatchNormalization(name='bn4')(conv4)\n",
    "        conv5 = tf.keras.layers.Conv2D(filters=48, kernel_size=(3, 1), strides=(1, 1), padding=\"same\", activation=\"relu\", name='conv5')(bn4)\n",
    "        bn5 = tf.keras.layers.BatchNormalization(name='bn5')(conv5)\n",
    "        pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name=\"pool2\")(bn5)\n",
    "        conv6 = tf.keras.layers.Conv2D(filters=48, kernel_size=(3, 1), strides=(1, 1), padding=\"same\", activation=\"relu\", name='conv6')(pool2)\n",
    "        resid_2 = tf.keras.layers.Add()([pool2, conv6])\n",
    "        relu2 = tf.keras.layers.ReLU(name=\"relu2\")(resid_2)\n",
    "        bn_res2 = tf.keras.layers.BatchNormalization(name='bn_res2')(relu2)\n",
    "\n",
    "        conv7 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 1), strides=(1, 1), padding=\"same\", activation=\"relu\", name='conv7')(bn_res2)\n",
    "        bn7 = tf.keras.layers.BatchNormalization(name='bn7')(conv7)\n",
    "        conv8 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 1), strides=(1, 1), padding=\"same\", activation=\"relu\", name='conv8')(bn7)\n",
    "        bn8 = tf.keras.layers.BatchNormalization(name='bn8')(conv8)\n",
    "        pool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool3')(bn8)\n",
    "        conv9 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 1), strides=(1, 1), padding=\"same\", activation=\"relu\", name='conv9')(pool3)\n",
    "        resid_3 = tf.keras.layers.Add()([pool3, conv9])\n",
    "        relu3 = tf.keras.layers.ReLU(name=\"relu3\")(resid_3)\n",
    "        bn_res3 = tf.keras.layers.BatchNormalization(name=\"bn_res3\")(relu3)\n",
    "\n",
    "        conv10 = tf.keras.layers.Conv2D(filters=96, kernel_size=(3, 1), strides=(1, 1), padding=\"same\", activation=\"relu\", name=\"conv10\")(bn_res3)\n",
    "        bn10 = tf.keras.layers.BatchNormalization(name=\"bn10\")(conv10)\n",
    "        conv11 = tf.keras.layers.Conv2D(filters=96, kernel_size=(3, 1), strides=(1, 1), padding=\"same\", activation=\"relu\", name=\"conv11\")(bn10)\n",
    "        bn11 = tf.keras.layers.BatchNormalization(name=\"bn11\")(conv11)\n",
    "        pool4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name=\"pool4\")(bn11)\n",
    "        conv12 = tf.keras.layers.Conv2D(filters=96, kernel_size=(3, 1), strides=(1, 1), padding=\"same\", activation=\"relu\", name=\"conv12\")(pool4)\n",
    "        resid_4 = tf.keras.layers.Add()([pool4, conv12])\n",
    "        relu4 = tf.keras.layers.ReLU(name=\"relu4\")(resid_4)\n",
    "        bn_res4 = tf.keras.layers.BatchNormalization(name=\"bn_res4\")(relu4)\n",
    "\n",
    "        conv13 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 1), strides=(1, 1), padding=\"same\", activation=\"relu\", name=\"conv13\")(bn_res4)\n",
    "        pool5 = tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name=\"pool5\")(conv13)\n",
    "        bn13 = tf.keras.layers.BatchNormalization(name=\"bn13\")(pool5)\n",
    "\n",
    "        conv14 = tf.keras.layers.Conv2D(filters=96, kernel_size=(3, 1), strides=(1, 1), padding=\"same\", activation=\"relu\", name=\"conv14\")(bn13)\n",
    "        bn14 = tf.keras.layers.BatchNormalization(name=\"bn14\")(conv14)\n",
    "\n",
    "        conv15 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 1), strides=(1, 1), padding=\"same\", activation=\"relu\", name=\"conv15\")(bn14)\n",
    "        bn15 = tf.keras.layers.BatchNormalization(name=\"bn15\")(conv15)\n",
    "\n",
    "        flatten_layer = tf.keras.layers.Flatten()(bn15)\n",
    "        \n",
    "        label_layer = tf.keras.layers.Dense(256, activation=\"relu\", name=\"dense\")(flatten_layer)\n",
    "        output_layer_1 = tf.keras.layers.Dense(2*5, activation=\"linear\", name=\"y1_output\")(label_layer)\n",
    "        output_layer_2 = tf.keras.layers.Dense(5*5, activation=\"linear\", name=\"y2_output\")(label_layer)\n",
    "\n",
    "        model = tf.keras.Model(inputs=input_layer, outputs=[output_layer_1, output_layer_2])\n",
    "        return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod = create_model(input_window_length=seq_len)\n",
    "mod = create_resnet_model(input_window_length=seq_len)\n",
    "\n",
    "dot_img_file = '/tmp/resnet_model.png'\n",
    "tf.keras.utils.plot_model(mod, to_file=dot_img_file, show_shapes=True)\n",
    "\n",
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_window_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiActivationLoss(tf.keras.Loss):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "                super().__init__(*args, **kwargs)\n",
    "\n",
    "                self.states_loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "        def call(self, y_true, y_pred):\n",
    "\n",
    "                pred_state = tf.reshape(y_pred, [-1, 5, 2])\n",
    "\n",
    "                pred_state_softmax = tf.nn.softmax(pred_state, axis=1)\n",
    "\n",
    "                loss_nll = self.states_loss(y_true, pred_state_softmax)\n",
    "\n",
    "                return loss_nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileLoss(tf.keras.Loss):\n",
    "        def __init__(self, quantiles=[0.0025,0.1, 0.5, 0.9, 0.975], *args, **kwargs):\n",
    "                super().__init__(*args, **kwargs)\n",
    "                self.quantiles = quantiles\n",
    "\n",
    "        def call(self, y_true, y_pred):\n",
    "                pred_rms = tf.reshape(y_pred, [-1, 5, 5])\n",
    "\n",
    "                targets = tf.expand_dims(y_true, axis=1)\n",
    "                targets = tf.repeat(targets, repeats=[5], axis=1)\n",
    "\n",
    "                quantiles = tf.convert_to_tensor(self.quantiles, dtype=tf.float32)\n",
    "                \n",
    "                error = tf.transpose((targets - pred_rms), perm=[0,2,1])\n",
    "                loss = tf.math.maximum(quantiles*error, (quantiles-1)*error)\n",
    "                return tf.reduce_mean(loss, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss={'y1_output' : MultiActivationLoss, 'y2_output' : QuantileLoss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2134/2577\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1:21\u001b[0m 184ms/step - loss: 0.4958"
     ]
    }
   ],
   "source": [
    "# Define the Keras TensorBoard callback.\n",
    "# Refer to https://stackoverflow.com/questions/56690089/how-to-graph-tf-keras-model-in-tensorflow-2-0\n",
    "\n",
    "logdir=\"../logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "checkpoint_path = f\"{logdir}/weights/cp.weights.h5\" \n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "mod.fit(dloader, epochs=1, callbacks=[tensorboard_callback, checkpoint_callback])\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

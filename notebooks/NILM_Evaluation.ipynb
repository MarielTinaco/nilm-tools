{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pathlib import Path\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.append('../src/')\n",
    "\n",
    "from sklearn import metrics\n",
    "import math\n",
    "from sklearn.metrics import f1_score, mean_absolute_error, mean_squared_error\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = Path.cwd().parent / \"results\" / \"results_1.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.load(RESULTS, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae(target, prediction):\n",
    "    return mean_absolute_error(target, prediction)\n",
    "\n",
    "def get_eac(target, prediction):\n",
    "    num = np.sum(np.abs(prediction-target))\n",
    "    den = (np.sum(target))\n",
    "    eac = 1 - (num/den)/2\n",
    "    return np.where(eac<0, 0, eac)\n",
    "\n",
    "def get_relative_error(target, prediction):\n",
    "    return np.mean(np.nan_to_num(np.abs(target - prediction) / np.maximum(target, prediction)))\n",
    "\n",
    "\n",
    "def get_nde(target, prediction):\n",
    "    return np.sum((target - prediction) ** 2) / np.sum((target ** 2))\n",
    "\n",
    "def compute_regress_metrics(target, prediction):\n",
    "    eac = get_eac(target, prediction)\n",
    "    mae = get_mae(target, prediction)\n",
    "    nade = get_nde(target, prediction)\n",
    "    \n",
    "   \n",
    "    metrics = OrderedDict([('EAC', eac),\n",
    "                        ('MAE', mae),\n",
    "                        ('NDE', nade)])\n",
    "    \n",
    "    metrics_dict = {}\n",
    "    metrics_dict['EAC'] = metrics[\"EAC\"]\n",
    "    metrics_dict['MAE'] = metrics[\"MAE\"]\n",
    "    metrics_dict['NDE'] = metrics[\"NDE\"]\n",
    "    \n",
    "    return metrics_dict\n",
    "\n",
    "appliances=['fridge', 'washer dryer','kettle', 'dish washer', 'microwave'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APP fridge\n",
      "        mean  std\n",
      "EAC    0.698  0.0\n",
      "MAE  178.014  0.0\n",
      "NDE    0.547  0.0\n",
      "\n",
      "APP washer dryer\n",
      "       mean  std\n",
      "EAC   0.807  0.0\n",
      "MAE  82.585  0.0\n",
      "NDE   0.229  0.0\n",
      "\n",
      "APP kettle\n",
      "        mean  std\n",
      "EAC    0.517  0.0\n",
      "MAE  288.333  0.0\n",
      "NDE    0.899  0.0\n",
      "\n",
      "APP dish washer\n",
      "        mean  std\n",
      "EAC    0.811  0.0\n",
      "MAE  123.780  0.0\n",
      "NDE    0.272  0.0\n",
      "\n",
      "APP microwave\n",
      "      mean  std\n",
      "EAC   0.73  0.0\n",
      "MAE  86.85  0.0\n",
      "NDE   0.48  0.0\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'example_f1_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[39mprint\u001b[39m(reg)\n\u001b[0;32m     25\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m ebf1 \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mExbF1\u001b[39m\u001b[39m\"\u001b[39m:example_f1_score(state, pred_state, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, per_sample\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mtolist()}\n\u001b[0;32m     27\u001b[0m ebf1 \u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_dict(ebf1, orient\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m ebf1\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(appliance_data\u001b[39m.\u001b[39mkeys())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'example_f1_score' is not defined"
     ]
    }
   ],
   "source": [
    "pred_power=results['pred_power']\n",
    "pred_state=results['pred_state']\n",
    "power=results['power']\n",
    "state=results['state']\n",
    "all_reg_results_mean = []\n",
    "all_reg_results_std = []\n",
    "regress = {}\n",
    "for idx, app in enumerate(appliances):\n",
    "    regress_results = []\n",
    "    for  q in range(5):\n",
    "        result = compute_regress_metrics(power[:,idx]*state[:,idx], pred_power[:,idx]*pred_state[:,idx])\n",
    "        result = pd.DataFrame.from_dict(result, orient=\"index\")\n",
    "        regress_results.append(result)\n",
    "    regress_results = pd.concat(regress_results, axis=1)\n",
    "    #regress_results.columns = list(appliance_data.keys())\n",
    "    #regress[app]=regress_results\n",
    "    print(\"APP\", app)\n",
    "    mean = regress_results.mean(1).round(3)\n",
    "    std  = regress_results.std(1).round(3)\n",
    "    reg=pd.concat([mean, std], axis=1)\n",
    "    reg.columns =[\"mean\", 'std']\n",
    "    all_reg_results_mean.append( mean)\n",
    "    all_reg_results_std.append(std)\n",
    "    print(reg)\n",
    "    print(\"\")\n",
    "#ebf1 = {\"ExbF1\":example_f1_score(state, pred_state, axis=0, per_sample=True).tolist()}\n",
    "#ebf1 =pd.DataFrame.from_dict(ebf1, orient=\"index\")\n",
    "#ebf1.columns = list(appliance_data.keys())\n",
    "print(\"\")\n",
    "#print(ebf1.round(3))\n",
    "\n",
    "\n",
    "#print(pd.DataFrame.from_dict(compute_metrics(state, pred_state), orient=\"index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
